{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1UL_O8HLWh2Auv-3WCRatLUN_9_pE7qEc",
      "authorship_tag": "ABX9TyMGF3KLxX6UGQA/Bh2kxI3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FerBarrey/webscrapingpython/blob/main/Linksvalidations_con_archivos_multiples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx requests pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEnn3eAzvaKp",
        "outputId": "e54b2a6c-57e9-4c13-f361-81b8a934642c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPCcsn6cEjIF",
        "outputId": "2f341528-fe78-441a-ff4f-9b8925c158f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zXOrClAIrQ7",
        "outputId": "51fbb23f-d000-41f2-d26f-ad37b587a755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo links.xlsx generado con enlaces aleatorios.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def generar_enlace_aleatorio():\n",
        "    \"\"\"Genera un enlace aleatorio, a veces válido, a veces inválido.\"\"\"\n",
        "    valido = random.choice([True, False])\n",
        "    if valido:\n",
        "        dominio = random.choice([\"www.google.com\", \"www.wikipedia.org\", \"www.ejemplo.com\"])\n",
        "        ruta = \"/pagina\" + str(random.randint(1, 10))\n",
        "        return \"https://\" + dominio + ruta\n",
        "    else:\n",
        "        # Genera un enlace inválido con mayor frecuencia\n",
        "        dominio = \"www.enlacenoexiste\" + str(random.randint(1, 1000)) + \".com\"\n",
        "        return \"https://\" + dominio + \"/ruta_invalida\"\n",
        "\n",
        "def generar_lista_enlaces(cantidad):\n",
        "    \"\"\"Genera una lista de enlaces aleatorios.\"\"\"\n",
        "    return [generar_enlace_aleatorio() for _ in range(cantidad)]\n",
        "\n",
        "# Generar 20 enlaces aleatorios\n",
        "enlaces = generar_lista_enlaces(20)\n",
        "\n",
        "# Crear un DataFrame de pandas con los enlaces\n",
        "df = pd.DataFrame({\"enlaces\": enlaces})\n",
        "\n",
        "# Guardar el DataFrame en un archivo Excel\n",
        "df.to_excel(\"links.xlsx\", index=False)\n",
        "\n",
        "print(\"Archivo links.xlsx generado con enlaces aleatorios.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisis de los links contenidos en el archivo links.xlsx\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "def verificar_enlace(enlace):\n",
        "  \"\"\"Verifica si un enlace es válido.\"\"\"\n",
        "  try:\n",
        "    response = requests.get(enlace, timeout=5)  # Timeout para evitar esperas largas\n",
        "    response.raise_for_status()  # Lanza una excepción para códigos de estado no exitosos (4xx o 5xx)\n",
        "    return \"Válido\"\n",
        "  except requests.exceptions.RequestException:\n",
        "    return \"Inválido\"\n",
        "\n",
        "# Cargar el archivo Excel\n",
        "df = pd.read_excel(\"links.xlsx\")\n",
        "\n",
        "# Verificar cada enlace y agregar la columna \"Válido\"\n",
        "df[\"Estado\"] = df[\"enlaces\"].apply(verificar_enlace)\n",
        "\n",
        "# Guardar los resultados en un nuevo archivo Excel\n",
        "df.to_excel(\"links_verificados.xlsx\", index=False)\n",
        "\n",
        "print(\"Archivo links_verificados.xlsx generado con los enlaces verificados.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5ROkEgYJrvI",
        "outputId": "d1e50e62-7cb0-49c1-f983-79807a2611a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo links_verificados.xlsx generado con los enlaces verificados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicio del proceso completo\n",
        "\n",
        "import docx\n",
        "!pip install python-docx\n",
        "import pandas as pd\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "# Configura el registro de errores\n",
        "logging.basicConfig(filename='error_log.txt', level=logging.ERROR,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def verificar_enlace(url):\n",
        "    try:\n",
        "        response = requests.head(url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            return 'Válido'\n",
        "        else:\n",
        "            return 'No Válido'\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error al verificar URL {url}: {e}\")\n",
        "        return 'Error'\n",
        "\n",
        "try:\n",
        "    doc = docx.Document('Informe.docx')\n",
        "    links_data = []\n",
        "\n",
        "    for paragraph in doc.paragraphs:\n",
        "        for run in paragraph.runs:\n",
        "            if run.hyperlink:\n",
        "                text = run.text\n",
        "                url = run.hyperlink.url\n",
        "                categoria = 'Otro'\n",
        "                if 'facebook.com' in url:\n",
        "                    categoria = 'Facebook'\n",
        "                elif 'twitter.com' in url or 'x.com' in url:\n",
        "                    categoria = 'X'\n",
        "                validez = verificar_enlace(url)\n",
        "                links_data.append([text, url, categoria, validez])\n",
        "\n",
        "    df = pd.DataFrame(links_data, columns=['Texto del Link', 'URL', 'Categoría', 'Validez'])\n",
        "    df.to_excel('linkinformeok.xlsx', index=False)\n",
        "    print(\"Archivo 'linkinformeok.xlsx' creado exitosamente.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    logging.error(\"Archivo 'Informe.docx' no encontrado.\")\n",
        "    print(\"Error: Archivo 'Informe.docx' no encontrado.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error inesperado: {e}\")\n",
        "    print(f\"Error inesperado: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhlGGB5dmNtj",
        "outputId": "88d0a3d9-93db-4a20-fc6f-ec6eb5f7a8ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error inesperado: Package not found at 'Informe.docx'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error inesperado: Package not found at 'Informe.docx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mc5XCbGQsHyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pruebo si se carga correctamente el archivo\n",
        "import os\n",
        "print(os.path.exists(\"Informe1.docx\") ) # Debe imprimir True si el archivo está presente\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIZf29N5yULk",
        "outputId": "daa3bddb-5a04-4252-873b-66996ce37b27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "\n",
        "try:\n",
        "    doc = Document(\"Informe1.docx\")\n",
        "    print(\"El archivo se cargó correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al abrir el documento: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPK4-SwYzK7B",
        "outputId": "b2061c5f-da49-4bc7-f7be-683b3d41a6c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El archivo se cargó correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from lxml import etree\n",
        "\n",
        "def get_hyperlinks(doc_path):\n",
        "    doc = Document(doc_path)\n",
        "    hyperlinks = []\n",
        "\n",
        "    for rel in doc.part.rels:\n",
        "        if \"hyperlink\" in doc.part.rels[rel].reltype:\n",
        "            hyperlinks.append(doc.part.rels[rel].target_ref)\n",
        "\n",
        "    return hyperlinks\n",
        "\n",
        "doc_path = \"Informe1.docx\"\n",
        "links = get_hyperlinks(doc_path)\n",
        "\n",
        "print(\"Hipervínculos extraídos:\", links)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig81Bx0O_GYu",
        "outputId": "e1d887d5-bdb7-43fd-d07b-47d423ecde8c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hipervínculos extraídos: ['https://www.c5n.com/sociedad/el-drama-los-vecinos-afectados-las-inundaciones-zona-sur-es-toda-la-vida-esto-n192775?fbclid=IwY2xjawIzxs1leHRuA2FlbQIxMQABHXdMs5R4tUGMo8xCHuepjMmCC-6Bu-cvXdimWayLi3cHomKTuj1_ugSe7A_aem_mqgEEd32hdKJpURcnY8E4g', 'https://ultimanoticiaba.com/2025/03/04/demoras-y-anegamientos-tras-intensas-lluvias-en-el-amba/', 'https://www.facebook.com/photo/?fbid=1860890784736076&set=a.118268865664952&locale=es_LA', 'https://www.facebook.com/photo?fbid=1093970502743887&set=pcb.1093970952743842', 'https://www.facebook.com/photo/?fbid=1207190034743817&set=pcb.1207190064743814&locale=es_LA', 'https://junin24.com/locales/mas-de-100-mm-de-lluvia-caida-en-menos-de-15-horas-el-saldo-de-calles-anegadas-bocas-de-tormenta-tapadas-y-el-enojo-de-los-vecinos/?fbclid=IwY2xjawIzyKpleHRuA2FlbQIxMQABHXE-26L6LmPuHWe8JUgxpU08ZYi6-0fvdZia9U6VP7eGFAiq_aN_q1ajvQ_aem_FIG7nuXJgsqYPWQ-KdOuow', 'https://datamagdalena.com.ar/nota/1916/-mas-de-136-mm-de-lluvia-en-bavio-bomberos-en-alerta-y-rescates-en-la-madrugada?fbclid=IwY2xjawIzvUdleHRuA2FlbQIxMQABHY1f2zHcXn1L9u6IwBIRC-6m3OsgsyODFTlyUNMPgK-Ln-O1chfi-HRiVQ_aem_AfLK4CovmTGOI2dU1LnTaw', 'https://www.facebook.com/25digital/videos/668530239067602', 'https://www.facebook.com/photo?fbid=1103236098480667&set=a.479513457519604', 'https://www.facebook.com/photo?fbid=1043054377846820&set=a.598790052273257', 'https://www.0223.com.ar/nota/2025-3-6-16-1-0-video-en-plena-tormenta-una-boca-de-tormenta-se-transformo-en-fuente-en-pleno-centro-de-mar-del-plata', 'https://www.facebook.com/photo/?fbid=1860941124731042&set=pcb.1860941351397686&locale=es_LA', 'https://www.clarin.com/sociedad/dramatico-relato-damnificado-inundaciones-zarate-agua-casa-llega-pecho-cuarta-vez-pasa_0_NYDGRrsieR.html', 'https://www.noticiastornquist.com.ar/2025/03/06/inundaciones-y-alertas-en-ciudades-del-sudoeste-bonaerense-1/?fbclid=IwY2xjawI3r5ZleHRuA2FlbQIxMQABHd708W8Vmy6lmYSmU0pGQJ3EfNn1dWiIzfHx5DDsSxvp7a5I5uYyFL18nA_aem_D2zSKLm-c7SKmSeTBEYLmA', 'https://www.facebook.com/photo/?fbid=1068442578661490&set=a.600134942158925', 'https://lavozdelpueblo.com.ar/los-arroyos-estan-muy-cargados-por-las-lluvias/?fbclid=IwY2xjawI35H5leHRuA2FlbQIxMAABHeoIp92iorNVuAOiaDniu89JDiA6HNIvUZ9qqAH3ST9ZIsmr3TQHoB8VWQ_aem_dxU1SsYj41Vk6OqAbIUsUA', 'http://www.bdh.acumar.gov.ar/bdh3/meteo/matanza/mb1.htm', 'https://www.facebook.com/photo/?fbid=1185904260204034&set=a.739869318140866&locale=es_LA', 'https://www.facebook.com/photo/?fbid=1191248299673190&set=pcb.1191076913023662', 'https://www.noticiastornquist.com.ar/2025/03/06/inundaciones-y-alertas-en-ciudades-del-sudoeste-bonaerense-1/?fbclid=IwY2xjawI3r5ZleHRuA2FlbQIxMQABHd708W8Vmy6lmYSmU0pGQJ3EfNn1dWiIzfHx5DDsSxvp7a5I5uYyFL18nA_aem_D2zSKLm-c7SKmSeTBEYLmA', 'https://www.facebook.com/photo?fbid=663821882870844&set=pcb.663822086204157', 'https://www.facebook.com/photo?fbid=1201200515340671&set=pcb.1201206358673420', 'https://www.facebook.com/photo?fbid=656637100386097&set=pcb.656637733719367', 'https://www.facebook.com/photo?fbid=1287649432830037&set=pcb.1287649459496701', 'https://x.com/ClimaLaPlata3/status/1896410160861741262', 'https://www.facebook.com/reel/585899114455364', 'https://www.facebook.com/photo/?fbid=1860874888070999&set=a.140229726802199&locale=es_LA', 'https://www.clarin.com/sociedad/dramatico-relato-damnificado-inundaciones-zarate-agua-casa-llega-pecho-cuarta-vez-pasa_0_NYDGRrsieR.html', 'https://www.facebook.com/photo?fbid=636165419022707&set=a.250635300909056', 'https://www.facebook.com/elvocerodemetan1/videos/9254467171316337', 'https://www.facebook.com/jotafmsierrasdecordoba/videos/496781943296079', 'https://www.lanacion.com.ar/economia/campo/muy-grave-casi-2-millones-de-hectareas-estan-bajo-el-agua-en-buenos-aires-y-alertan-por-importantes-nid09032025/', 'https://www.facebook.com/photo/?fbid=663358229695003&set=pcb.663358299694996&locale=es_LA', 'https://www.lanacion.com.ar/sociedad/cuando-deja-de-llover-en-48-horas-cayo-casi-la-misma-cantidad-de-agua-en-el-amba-que-en-todo-febrero-nid03032025/', 'https://infocielo.com/politica-y-economia/familias-asistidas-y-campos-anegados-por-las-lluvias-provincia-sigue-de-cerca-lo-que-pasa-en-dos-ciudades-bonaerenses', 'https://www.facebook.com/divulgandonoticiaszc/videos/1839938736753658', 'https://infocielo.com/sociedad/un-rayo-atraveso-las-paredes-de-una-casa-en-miramar-mira-el-video', 'https://www.facebook.com/photo?fbid=9835103703207073&set=pcb.2087986481698149', 'https://www.facebook.com/photo?fbid=1103236098480667&set=a.479513457519604']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from docx import Document\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def verificar_enlace(url):\n",
        "    \"\"\"Verifica si una URL es accesible con un timeout corto.\"\"\"\n",
        "    try:\n",
        "        response = requests.head(url, allow_redirects=True, timeout=1.5)  # Usa HEAD para mayor velocidad\n",
        "        return 'Válido' if response.status_code == 200 else 'No Válido'\n",
        "    except requests.exceptions.RequestException:\n",
        "        return 'Error'\n",
        "\n",
        "def get_hyperlinks(doc_path):\n",
        "    \"\"\"Extrae hipervínculos de un documento Word (.docx).\"\"\"\n",
        "    doc = Document(doc_path)\n",
        "    return [doc.part.rels[rel].target_ref for rel in doc.part.rels if \"hyperlink\" in doc.part.rels[rel].reltype]\n",
        "\n",
        "def procesar_links(doc_path):\n",
        "    \"\"\"Extrae hipervínculos, los categoriza y verifica su validez en paralelo.\"\"\"\n",
        "    hyperlinks = get_hyperlinks(doc_path)\n",
        "    links_data = []\n",
        "\n",
        "    # Clasificar los enlaces rápidamente\n",
        "    categorias = ['Facebook' if 'facebook.com' in url else\n",
        "                  'X' if 'twitter.com' in url or 'x.com' in url else 'Otro'\n",
        "                  for url in hyperlinks]\n",
        "\n",
        "    # Verificar los enlaces en paralelo (para mayor velocidad)\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        validez_list = list(executor.map(verificar_enlace, hyperlinks))\n",
        "\n",
        "    # Combinar los datos en una lista final\n",
        "    links_data = list(zip(hyperlinks, categorias, validez_list))\n",
        "    return links_data\n",
        "\n",
        "# Ejecutar la extracción y guardar en Excel\n",
        "try:\n",
        "    doc_path = 'Informe1.docx'\n",
        "    links_data = procesar_links(doc_path)\n",
        "\n",
        "    if links_data:\n",
        "        df = pd.DataFrame(links_data, columns=['URL', 'Categoría', 'Validez'])\n",
        "        df.to_excel('linkinformeok.xlsx', index=False)\n",
        "        print(\"✅ Archivo 'linkinformeok.xlsx' creado exitosamente.\")\n",
        "    else:\n",
        "        print(\"⚠️ No se encontraron hipervínculos en el documento.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Error: Archivo 'Informe1.docx' no encontrado.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error inesperado: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TDfO8a0PZOc",
        "outputId": "af480dc0-e3d8-470f-9383-8ac83ee9d59f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'linkinformeok.xlsx' creado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy\n",
        "!pip install python-docx\n",
        "!pip install nltk\n",
        "import pandas as pd\n",
        "import requests\n",
        "from docx import Document\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from bs4 import BeautifulSoup\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "\n",
        "#Descargo recursos NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the 'punkt_tab' data package( esto me asegura que lea en español)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "def verificar_enlace(url):\n",
        "    \"\"\"Verifica si una URL es accesible con un timeout corto.\"\"\"\n",
        "    try:\n",
        "        response = requests.head(url, allow_redirects=True, timeout=1.5)\n",
        "        return 'Válido' if response.status_code == 200 else 'No Válido'\n",
        "    except requests.exceptions.RequestException:\n",
        "        return 'Error'\n",
        "\n",
        "def get_hyperlinks(doc_path):\n",
        "    \"\"\"Extrae hipervínculos de un documento Word (.docx).\"\"\"\n",
        "    doc = Document(doc_path)\n",
        "    return [doc.part.rels[rel].target_ref for rel in doc.part.rels if \"hyperlink\" in doc.part.rels[rel].reltype]\n",
        "\n",
        "def extraer_texto_y_resumir(url, num_sentences=3):\n",
        "    \"\"\"Extrae el texto de la noticia desde la URL y genera un resumen relevante.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=3)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extraer el texto principal de la noticia\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_text = ' '.join([p.get_text() for p in paragraphs if len(p.get_text()) > 20])\n",
        "\n",
        "        if not full_text:\n",
        "            return \"No se pudo extraer el contenido.\"\n",
        "\n",
        "        # Resumir el texto usando Sumy LSA (asegurando compatibilidad con el idioma)\n",
        "        parser = PlaintextParser.from_string(full_text, Tokenizer(\"es\"))  # Alternativa: \"spanish\"\n",
        "        summarizer = LsaSummarizer()\n",
        "        summary = summarizer(parser.document, num_sentences)\n",
        "\n",
        "        return ' '.join([str(sentence) for sentence in summary])\n",
        "\n",
        "    except requests.exceptions.RequestException:\n",
        "        return \"No se pudo acceder a la noticia.\"\n",
        "\n",
        "def procesar_links(doc_path):\n",
        "    \"\"\"Extrae hipervínculos, los categoriza, verifica su validez y resume noticias.\"\"\"\n",
        "    hyperlinks = get_hyperlinks(doc_path)\n",
        "\n",
        "    # Clasificar los enlaces rápidamente\n",
        "    categorias = ['Facebook' if 'facebook.com' in url else\n",
        "                  'X' if 'twitter.com' in url or 'x.com' in url else 'Otro'\n",
        "                  for url in hyperlinks]\n",
        "\n",
        "    # Verificar los enlaces en paralelo (para mayor velocidad)\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        validez_list = list(executor.map(verificar_enlace, hyperlinks))\n",
        "\n",
        "    # Extraer el resumen de cada noticia en paralelo\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        resumen_list = list(executor.map(extraer_texto_y_resumir, hyperlinks))\n",
        "\n",
        "    # Combinar los datos en una lista final\n",
        "    links_data = list(zip(hyperlinks, categorias, validez_list, resumen_list))\n",
        "    return links_data\n",
        "\n",
        "# Ejecutar la extracción y guardar en Excel\n",
        "try:\n",
        "    doc_path = 'Informe1.docx'\n",
        "    links_data = procesar_links(doc_path)\n",
        "\n",
        "    if links_data:\n",
        "        df = pd.DataFrame(links_data, columns=['URL', 'Categoría', 'Validez', 'Resumen'])\n",
        "        df.to_excel('linkinformeok.xlsx', index=False)\n",
        "        print(\"✅ Archivo 'linkinformeok.xlsx' creado exitosamente con resúmenes.\")\n",
        "    else:\n",
        "        print(\"⚠️ No se encontraron hipervínculos en el documento.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ Error: Archivo 'Informe.docx' no encontrado.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error inesperado: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxJXAaHCUTGz",
        "outputId": "00f2fb0d-8b6d-44f3-d5af-f9835f9b8d78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.7.14)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=fe3acd499a3c1ab5518c7f240b7d314797a1b1e2223893255a09c38c19cfc55f\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=3890bf4bf8c0322b92fb546080c8cbdf83281a76c4a35b0dad62a050641cca7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'linkinformeok.xlsx' creado exitosamente con resúmenes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#VERIFICO SI EL ARCHIVO se puede abrir correctamente\n",
        "from docx import Document\n",
        "!pip install python-docx\n",
        "\n",
        "try:\n",
        "    ruta_archivo = 'Informe1.docx'\n",
        "    doc = Document(ruta_archivo)  # Abrir el archivo\n",
        "    print(\"Archivo abierto correctamente.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error inesperado: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVeNo1uR6Wuz",
        "outputId": "8f125cdc-3431-4c63-ecaf-c79feeb8c9c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Archivo abierto correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verifico si el idioma es compatible con el tokenizador\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texto = \"Hola, ¿cómo estás?\"\n",
        "tokens = word_tokenize(texto, language='spanish')\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9NBovf49chp",
        "outputId": "61cf09e7-0af5-4441-8d02-7ab869f35b0b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', ',', '¿cómo', 'estás', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verifico si el tokenizador esta instalado correctamente\n",
        "\n",
        "import nltk\n",
        "print(nltk.__version__)\n",
        "\n",
        "\"\"\"el tokenizador cumple un papel fundamental en el proceso de resumen de texto que utiliza la biblioteca sumy. Específicamente, el Tokenizer(\"spanish\") se utiliza dentro de la función extraer_texto_y_resumir.Esto es lo que hace:Divide el texto: El tokenizador toma el full_text extraído de una página web y lo divide en unidades individuales, que suelen ser palabras y signos de puntuación. Procesamiento específico del idioma: Al especificar \"spanish\", el tokenizador se configura para entender las reglas y estructuras del idioma español. Esto es importante para identificar correctamente los límites de las palabras, manejar la puntuación y, potencialmente, reconocer frases o contracciones comunes en español.\"\"\"\n",
        "#\"\"\"Prepara para el resumen: La salida del tokenizador (una lista de tokens) es utilizada posteriormente por el LsaSummarizer. Los algoritmos de resumen como LSA funcionan analizando las relaciones entre palabras y oraciones. Una tokenización adecuada es esencial para que estos algoritmos procesen el texto con precisión e identifiquen las oraciones más importantes para el resumen.\n",
        "#En resumen, el tokenizador prepara el texto para el algoritmo de resumen convirtiéndolo a un formato que el algoritmo pueda entender y procesar eficazmente, teniendo en cuenta el idioma específico del texto"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "A_djfM94-M3Y",
        "outputId": "c64597c1-66e6-40a4-8703-c01fd9f53f07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'el tokenizador cumple un papel fundamental en el proceso de resumen de texto que utiliza la biblioteca sumy. Específicamente, el Tokenizer(\"spanish\") se utiliza dentro de la función extraer_texto_y_resumir.Esto es lo que hace:Divide el texto: El tokenizador toma el full_text extraído de una página web y lo divide en unidades individuales, que suelen ser palabras y signos de puntuación. Procesamiento específico del idioma: Al especificar \"spanish\", el tokenizador se configura para entender las reglas y estructuras del idioma español. Esto es importante para identificar correctamente los límites de las palabras, manejar la puntuación y, potencialmente, reconocer frases o contracciones comunes en español.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d63c17ce"
      },
      "source": [
        "# Modifico el codigo anterior para tratamiento multiple de archivos de entrada(docx) y salida( excel)\n",
        "Modifica el código  para iterar sobre varios documentos de Word (p. ej., \"Informe1.docx\", \"Informe2.docx\", \"Informe3.docx\") y procesa cada uno individualmente con la función `procesar_links`. Para cada documento de entrada, guarda el resultado en un archivo de Excel con un nombre único (p. ej., \"linkinformeok1.xlsx\", \"linkinformeok2.xlsx\", \"linkinformeok3.xlsx\"). .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5eeabd5"
      },
      "source": [
        "## Modificar la función `procesar links`\n",
        "\n",
        "### Subtarea:\n",
        "Actualizar la función `procesar_links` para que acepte una lista de rutas de documentos como entrada en lugar de una sola ruta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02570d1e"
      },
      "source": [
        "def procesar_links(doc_paths):\n",
        "    \"\"\"Extrae hipervínculos from multiple Word documents, categorizes, verifies validity, and summarizes news.\"\"\"\n",
        "    all_links_dataframes = []\n",
        "\n",
        "    for doc_path in doc_paths:\n",
        "        try:\n",
        "            hyperlinks = get_hyperlinks(doc_path)\n",
        "            links_data = []\n",
        "\n",
        "            # Clasificar los enlaces rápidamente\n",
        "            categorias = ['Facebook' if 'facebook.com' in url else\n",
        "                          'X' if 'twitter.com' in url or 'x.com' in url else 'Otro'\n",
        "                          for url in hyperlinks]\n",
        "\n",
        "            # Verificar los enlaces en paralelo (para mayor velocidad)\n",
        "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "                validez_list = list(executor.map(verificar_enlace, hyperlinks))\n",
        "\n",
        "            # Extraer el resumen de cada noticia en paralelo\n",
        "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "                resumen_list = list(executor.map(extraer_texto_y_resumir, hyperlinks))\n",
        "\n",
        "            # Combinar los datos en una lista final\n",
        "            links_data = list(zip(hyperlinks, categorias, validez_list, resumen_list))\n",
        "\n",
        "            if links_data:\n",
        "                df = pd.DataFrame(links_data, columns=['URL', 'Categoría', 'Validez', 'Resumen'])\n",
        "                all_links_dataframes.append(df)\n",
        "            else:\n",
        "                print(f\"⚠️ No se encontraron hipervínculos en el documento: {doc_path}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"❌ Error: Archivo '{doc_path}' no encontrado.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error inesperado al procesar {doc_path}: {e}\")\n",
        "\n",
        "    return all_links_dataframes"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "108d31e4"
      },
      "source": [
        "## Iterar documentos\n",
        "\n",
        "### Subtarea:\n",
        "Modifica el bloque de ejecución principal para obtener una lista de todos los documentos de Word que se procesarán.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec978239"
      },
      "source": [
        "document_list = ['Informe1.docx', 'Informe2.docx', 'Informe3.docx', 'Informe4.docx','Informe5.docx']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4782b6b"
      },
      "source": [
        "## Procesa cada documento alojados en documet_list\n",
        "\n",
        "### Subtarea:\n",
        "Recorre la lista de documentos, llama a la función procesar_links modificada para cada documento y guarda los resultados en un archivo de Excel con un nombre único ( linkinformeok1.xlsx, linkinformeok2.xlsx). (e.g., `linkinformeok1.xlsx`, `linkinformeok2.xlsx`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98e708e1",
        "outputId": "eaffe316-9b33-4141-94bd-0a761cd24234"
      },
      "source": [
        "document_list = ['Informe1.docx', 'Informe2.docx', 'Informe3.docx','Informe4.docx','Informe5.docx' ]\n",
        "\n",
        "# Se ejecuta el proceso sobre el listado de documentos\n",
        "all_processed_dataframes = procesar_links(document_list)\n",
        "\n",
        "# Se guarda cada nuevo archivo excel con un nombre y su numero correspondiente\n",
        "for i, df in enumerate(all_processed_dataframes):\n",
        "    output_filename = f'linkinformeok{i+1}.xlsx'\n",
        "    try:\n",
        "        df.to_excel(output_filename, index=False)\n",
        "        print(f\"✅ Archivo '{output_filename}' creado exitosamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error al guardar el archivo '{output_filename}': {e}\")\n",
        "\n",
        "if not all_processed_dataframes:\n",
        "    print(\"⚠️ No se procesaron documentos o no se encontraron hipervínculos válidos en ninguno.\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'linkinformeok1.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok2.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok3.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok4.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok5.xlsx' creado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "002ceb94",
        "outputId": "59d5639c-ba9e-4820-d5a1-389554cced3c"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from docx import Document\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from bs4 import BeautifulSoup\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data (ensure they are downloaded)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "def verificar_enlace(url):\n",
        "    \"\"\"Verifica si una URL es accesible con un timeout corto.\"\"\"\n",
        "    try:\n",
        "        response = requests.head(url, allow_redirects=True, timeout=1.5)  # Usa HEAD para mayor speed\n",
        "        return 'Válido' if response.status_code == 200 else 'No Válido'\n",
        "    except requests.exceptions.RequestException:\n",
        "        return 'Error'\n",
        "\n",
        "def get_hyperlinks(doc_path):\n",
        "    \"\"\"Extrae hipervínculos de un documento Word (.docx).\"\"\"\n",
        "    doc = Document(doc_path)\n",
        "    return [doc.part.rels[rel].target_ref for rel in doc.part.rels if \"hyperlink\" in doc.part.rels[rel].reltype]\n",
        "\n",
        "def extraer_texto_y_resumir(url, num_sentences=3):\n",
        "    \"\"\"Extrae el texto de la noticia desde la URL and genera un resumen relevante.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=3)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extraer el texto principal de la noticia (adaptable según la web)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_text = ' '.join([p.get_text() for p in paragraphs if len(p.get_text()) > 20])\n",
        "\n",
        "        if not full_text:\n",
        "            return \"No se pudo extraer el contenido.\"\n",
        "\n",
        "        # Resumir el texto usando Sumy LSA\n",
        "        parser = PlaintextParser.from_string(full_text, Tokenizer(\"spanish\"))\n",
        "        summarizer = LsaSummarizer()\n",
        "        summary = summarizer(parser.document, num_sentences)\n",
        "\n",
        "        return ' '.join([str(sentence) for sentence in summary])\n",
        "\n",
        "    except requests.exceptions.RequestException:\n",
        "        return \"No se pudo acceder a la noticia.\"\n",
        "\n",
        "def procesar_links(doc_paths):\n",
        "    \"\"\"Extrae hipervínculos from multiple Word documents, categorizes, verifies validity, and summarizes news.\"\"\"\n",
        "    all_links_dataframes = []\n",
        "\n",
        "    for doc_path in doc_paths:\n",
        "        try:\n",
        "            hyperlinks = get_hyperlinks(doc_path)\n",
        "            links_data = []\n",
        "\n",
        "            # Clasificar los enlaces rápidamente\n",
        "            categorias = ['Facebook' if 'facebook.com' in url else\n",
        "                          'X' if 'twitter.com' in url or 'x.com' in url else 'Otro'\n",
        "                          for url in hyperlinks]\n",
        "\n",
        "            # Verificar los enlaces en paralelo (para mayor velocidad)\n",
        "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "                validez_list = list(executor.map(verificar_enlace, hyperlinks))\n",
        "\n",
        "            # Extraer el resumen de cada noticia en paralelo\n",
        "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "                resumen_list = list(executor.map(extraer_texto_y_resumir, hyperlinks))\n",
        "\n",
        "            # Combinar los datos en una lista final\n",
        "            links_data = list(zip(hyperlinks, categorias, validez_list, resumen_list))\n",
        "\n",
        "            if links_data:\n",
        "                df = pd.DataFrame(links_data, columns=['URL', 'Categoría', 'Validez', 'Resumen'])\n",
        "                all_links_dataframes.append(df)\n",
        "            else:\n",
        "                print(f\"⚠️ No se encontraron hipervínculos en el documento: {doc_path}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"❌ Error: Archivo '{doc_path}' no encontrado.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error inesperado al procesar {doc_path}: {e}\")\n",
        "\n",
        "    return all_links_dataframes\n",
        "\n",
        "# List of documents to process\n",
        "document_list = ['Informe1.docx', 'Informe2.docx', 'Informe3.docx', 'Informe4.docx','Informe5.docx']\n",
        "\n",
        "# Execute the processing for the list of documents\n",
        "all_processed_dataframes = procesar_links(document_list)\n",
        "\n",
        "# Save each dataframe to a uniquely named Excel file\n",
        "for i, df in enumerate(all_processed_dataframes):\n",
        "    output_filename = f'linkinformeok{i+1}.xlsx'\n",
        "    try:\n",
        "        df.to_excel(output_filename, index=False)\n",
        "        print(f\"✅ Archivo '{output_filename}' creado exitosamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error al guardar el archivo '{output_filename}': {e}\")\n",
        "\n",
        "if not all_processed_dataframes:\n",
        "    print(\"⚠️ No se procesaron documentos o no se encontraron hipervínculos válidos en ninguno.\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'linkinformeok1.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok2.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok3.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok4.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok5.xlsx' creado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og66yWdFxt1r",
        "outputId": "109dbc9b-b1f2-49a8-ab77-c16ecf7ebaf8"
      },
      "source": [
        "!pip install python-docx"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4af03ac1",
        "outputId": "b432251d-e40f-47ae-f007-d67d1337cfcc"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from docx import Document\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from bs4 import BeautifulSoup\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data (ensure they are downloaded)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "def verificar_enlace(url):\n",
        "    \"\"\"Verifica si una URL es accesible con un timeout corto.\"\"\"\n",
        "    try:\n",
        "        response = requests.head(url, allow_redirects=True, timeout=1.5)  # Usa HEAD para mayor speed\n",
        "        return 'Válido' if response.status_code == 200 else 'No Válido'\n",
        "    except requests.exceptions.RequestException:\n",
        "        return 'Error'\n",
        "\n",
        "def get_hyperlinks(doc_path):\n",
        "    \"\"\"Extrae hipervínculos de un documento Word (.docx).\"\"\"\n",
        "    doc = Document(doc_path)\n",
        "    return [doc.part.rels[rel].target_ref for rel in doc.part.rels if \"hyperlink\" in doc.part.rels[rel].reltype]\n",
        "\n",
        "def extraer_texto_y_resumir(url, num_sentences=3):\n",
        "    \"\"\"Extrae el texto de la noticia desde la URL and genera un resumen relevante.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=3)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extraer el texto principal de la noticia (adaptable según la web)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_text = ' '.join([p.get_text() for p in paragraphs if len(p.get_text()) > 20])\n",
        "\n",
        "        if not full_text:\n",
        "            return \"No se pudo extraer el contenido.\"\n",
        "\n",
        "        # Resumir el texto usando Sumy LSA\n",
        "        parser = PlaintextParser.from_string(full_text, Tokenizer(\"spanish\"))\n",
        "        summarizer = LsaSummarizer()\n",
        "        summary = summarizer(parser.document, num_sentences)\n",
        "\n",
        "        return ' '.join([str(sentence) for sentence in summary])\n",
        "\n",
        "    except requests.exceptions.RequestException:\n",
        "        return \"No se pudo acceder a la noticia.\"\n",
        "\n",
        "def procesar_links(doc_paths):\n",
        "    \"\"\"Extrae hipervínculos from multiple Word documents, categorizes, verifies validity, and summarizes news.\"\"\"\n",
        "    all_links_dataframes = []\n",
        "\n",
        "    for doc_path in doc_paths:\n",
        "        try:\n",
        "            hyperlinks = get_hyperlinks(doc_path)\n",
        "            links_data = []\n",
        "\n",
        "            # Clasificar los enlaces rápidamente\n",
        "            categorias = ['Facebook' if 'facebook.com' in url else\n",
        "                          'X' if 'twitter.com' in url or 'x.com' in url else 'Otro'\n",
        "                          for url in hyperlinks]\n",
        "\n",
        "            # Verificar los enlaces en paralelo (para mayor velocidad)\n",
        "            with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "                validez_list = list(executor.map(verificar_enlace, hyperlinks))\n",
        "\n",
        "            # Extraer el resumen de cada noticia en paralelo\n",
        "            with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "                resumen_list = list(executor.map(extraer_texto_y_resumir, hyperlinks))\n",
        "\n",
        "            # Combinar los datos en una lista final\n",
        "            links_data = list(zip(hyperlinks, categorias, validez_list, resumen_list))\n",
        "\n",
        "            if links_data:\n",
        "                df = pd.DataFrame(links_data, columns=['URL', 'Categoría', 'Validez', 'Resumen'])\n",
        "                all_links_dataframes.append(df)\n",
        "            else:\n",
        "                print(f\"⚠️ No se encontraron hipervínculos en el documento: {doc_path}\")\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"❌ Error: Archivo '{doc_path}' no encontrado.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error inesperado al procesar {doc_path}: {e}\")\n",
        "\n",
        "    return all_links_dataframes\n",
        "\n",
        "# List of documents to process\n",
        "document_list = ['Informe1.docx', 'Informe2.docx', 'Informe3.docx', 'Informe4.docx','Informe5.docx']\n",
        "\n",
        "# Execute the processing for the list of documents\n",
        "all_processed_dataframes = procesar_links(document_list)\n",
        "\n",
        "# Save each dataframe to a uniquely named Excel file\n",
        "for i, df in enumerate(all_processed_dataframes):\n",
        "    output_filename = f'linkinformeok{i+1}.xlsx'\n",
        "    try:\n",
        "        df.to_excel(output_filename, index=False)\n",
        "        print(f\"✅ Archivo '{output_filename}' creado exitosamente.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error al guardar el archivo '{output_filename}': {e}\")\n",
        "\n",
        "if not all_processed_dataframes:\n",
        "    print(\"⚠️ No se procesaron documentos o no se encontraron hipervínculos válidos en ninguno.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Archivo 'linkinformeok1.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok2.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok3.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok4.xlsx' creado exitosamente.\n",
            "✅ Archivo 'linkinformeok5.xlsx' creado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkWM_iD6x2cD",
        "outputId": "c951b526-9971-4511-80b4-ed42a94f8fa2"
      },
      "source": [
        "!pip install sumy #con esta libreria se resumen los textos."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sumy in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.11/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.11/dist-packages (from sumy) (24.6.1)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uytZ_sUDBbXv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}